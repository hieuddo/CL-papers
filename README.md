# CL-papers

## SURVEY
* [Overview, history, and connections to other fields] Continual Lifelong Learning with Neural Networks: A Review  
  https://arxiv.org/abs/1802.07569

* [Benchmarks and SOTA] A continual learning survey: Defying forgetting in classification tasks  
  https://arxiv.org/abs/1909.08383

* [Thesis, useful background, CV focused] Continual learning in neural networks
  https://arxiv.org/abs/1910.02718

* [Theory thesis] Theoretical foundations of multi-task lifelong learning  
  https://research-explorer.app.ist.ac.at/download/1126/5056/IST-2017-776-v1%2B1_Pentina_Thesis_2016.pdf


## CLASSICAL PAPERS
### NOVEL & INFLUENTIAL IDEAS
* Learning without forgetting (ECCV2016)  
* Progressive neural networks (arxiv2016)  
* PathNet: Evolution Channels Gradient Descent in Super Neural Networks [arxiv2017]  
* Gradient Episodic Memory for Continual Learning (NIPS2017)  
* Continual Learning with Deep Generative Replay (NIPS2017)  
* Overcoming catastrophic forgetting in neural networks (EWC) (PNAS2017)  
* iCaRL: Incremental Classifier and Representation Learning (CVPR2017)  
* Continual Learning Through Synaptic Intelligence (ICML2017)  
* Lifelong Learning with Dynamically Expandable Networks (ICLR2018)  
* FearNet: Brain-Inspired Model for Incremental Learning (ICLR2018)  
* Variational Continual Learning (ICLR2018)  
* Overcoming Catastrophic Forgetting with Hard Attention to the Task (ICML2018)  
* Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting (NIPS2018)  
* Reinforced Continual Learning (NIPS2018)   
* Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference (ICLR2019)  
* [First NLP papers] Compositional Continual Language Learning (ICLR2020)  
* [First NLP papers] LAMOL: LAnguage MOdeling for Lifelong Language Learning (ICLR2020)  
* A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning (ICLR2020)  
* Continual learning with hypernetworks (ICLR2020)  
* Continual Learning with Adaptive Weights (CLAW) (ICLR2020)  
* Coresets via Bilevel Optimization for Continual Learning and Streaming (NeurIPS2020)  
* Calibrating CNNs for Lifelong Learning (NeurIPS2020)  
* Continual Deep Learning by Functional Regularisation of Memorable Past (NeurIPS2020)   

## THEORETICAL PAPERS
### HARDCORE
* A PAC-Bayesian Bound for Lifelong Learning (ICML2014)  
* Lifelong learning with non-iid tasks (NIPS2015)  
* Lifelong learning with weighted majority votes (NIPS2016)  
* Optimal Continual Learning has Perfect Memory and is NP-HARD (ICML2020)  

### PAPERS THAT BASED FROM PRINCIPLES FROM CLASSICAL/THEORETICAL PAPERS
* Overcoming Catastrophic Forgetting by Incremental Moment Matching (NIPS2017)  
* Lifelong Learning with Dynamically Expandable Networks (ICLR2018)  
* PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning (CVPR2018)  
* Lifelong Learning via Progressive Distillation and Retrospection (ECCV2018)  
* Memory Aware Synapses: Learning what (not) to forget (ECCV2018)  
* Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights (ECCV2018)  
* End-to-End Incremental Learning (ECCV2018)   
* Overcoming Catastrophic Forgetting via Model Adaptation (ICLR2019)  
* [Good code base] Efficient Lifelong Learning with A-GEM (ICLR2019)  
* Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting (ICML2019)  
* Task-Free Continual Learning (CVPR2019)  
* Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning (CVPR2019)   
* Learning Without Memorizing (CVPR2019)   
* Learning a Unified Classifier Incrementally via Rebalancing (CVPR2019)   
* Large Scale Incremental Learning (CVPR2019)  
* Continual Learning by Asymmetric Loss Approximation with Single-Side Overestimation (ICCV2019)  
* Online Continual Learning with Maximal Interfered Retrieval (NeurIPS2019)  
* Random Path Selection for Incremental Learning (NeurIPS2019)  
* Incremental Learning Using Conditional Adversarial Networks (ICCV2019)  
* IL2M: Class Incremental Learning With Dual Memory (ICCV2019)  
* Complementary Learning for Overcoming Catastrophic Forgetting Using Experience Replay (IJCAI2019)  
* Compacting, Picking and Growing for Unforgetting Continual Learning (NeurIPS2019)  
* Mnemonics Training: Multi-Class Incremental Learning without Forgetting (CVPR2020)  
* [Meta CL] iTAML : An Incremental Task-Agnostic Meta-learning Approach (CVPR2020)  
* Conditional Channel Gated Networks for Task-Aware Continual Learning (CVPR2020)  
* Incremental Learning In Online Scenario (CVPR2020)  
* Online Continual Learning under Extreme Memory Constraints (ECCV2020)  
* REMIND Your Neural Network to Prevent Catastrophic Forgetting (ECCV2020)  
* Adversarial Continual Learning (ECCV2020)  
* Dark Experience for General Continual Learning: a Strong, Simple Baseline (NeurIPS2020)   
* Using Hindsight to Anchor Past Knowledge in Continual Learning(AAAI, 2021)  


## ANALYSIS PAPERS
### GOOD INSIGHTS
* Revisiting distillation and incremental classifier learning (ACCV2018)  
* Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence (ECCV2018)  
* Measuring Catastrophic Forgetting in Neural Networks (AAAI2018)  
* A comprehensive, application-oriented study of catastrophic forgetting in DNNs (ICLR2019)  
* [Nice summary paper, but the proposed method doesnâ€™t work] GDumb: A Simple Approach that Questions Our Progress in Continual Learning (ECCV2020)   
* Understanding the Role of Training Regimes in Continual Learning (NeurIPS2020)   
* Anatomy of catastrophic forgetting: Hidden representations and task semantics (ICLR2021)  
* Optimization and Generalization of Regularization-Based Continual Learning: a Loss Approximation Viewpoint (arxiv2021)  

## CONTINUAL LEARNING & OTHER SETTINGS
### META CONTINUAL LEARNING, GAN AND FORGETTING, CL IN NLP, RS, ETC.
* Meta-learning representations for continual learning (NIPS2019)  
* Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning (NeurIPS2020)  
* GAN Memory with No Forgetting (NeurIPS2020)   
* Improving generalization and stability of generative adversarial networks (IJCNN2019)  
* Catastrophic forgetting and mode collapse in GANs (IJCNN2020)  
* Meta-Consolidation for Continual Learning (NeurIPS2020)   
* Distill and Replay for Continual Language Learning(COLING, 2020)  
* Continual Learning for Natural Language Generation in Task-oriented Dialog Systems(EMNLP, 2020) (Findings of ACL)  
* ADER: Adaptively Distilled Exemplar Replay Towards Continual Learning for Session-based Recommendation(RecSys, 2020) (Short paper)  
* One Person, One Model, One World: Learning Continual User Representation without Forgetting (arxiv2020)  
* Addressing catastrophic forgetting for medical domain expansion (arxiv2021)
