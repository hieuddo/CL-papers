# CL-papers

## SURVEY
[Overview, history, and connections to other fields] Continual Lifelong Learning with Neural Networks: A Review
https://arxiv.org/abs/1802.07569

[Benchmarks and SOTA] A continual learning survey: Defying forgetting in classification tasks
https://arxiv.org/abs/1909.08383

[Thesis, useful background, CV focused] Continual learning in neural networks
https://arxiv.org/abs/1910.02718

[Theory thesis] Theoretical foundations of multi-task lifelong learning
https://research-explorer.app.ist.ac.at/download/1126/5056/IST-2017-776-v1%2B1_Pentina_Thesis_2016.pdf


## CLASSICAL PAPERS
### NOVEL & INFLUENTIAL IDEAS
Learning without forgetting (ECCV2016)  
Progressive neural networks (arxiv2016)  
PathNet: Evolution Channels Gradient Descent in Super Neural Networks [arxiv2017]  
Gradient Episodic Memory for Continual Learning (NIPS2017)  
Continual Learning with Deep Generative Replay (NIPS2017)  
Overcoming catastrophic forgetting in neural networks (EWC) (PNAS2017)  
iCaRL: Incremental Classifier and Representation Learning (CVPR2017)  
Continual Learning Through Synaptic Intelligence (ICML2017)  
Lifelong Learning with Dynamically Expandable Networks (ICLR2018)  
FearNet: Brain-Inspired Model for Incremental Learning (ICLR2018)  
Variational Continual Learning (ICLR2018)  
Overcoming Catastrophic Forgetting with Hard Attention to the Task (ICML2018)  
Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting (NIPS2018)  
Reinforced Continual Learning (NIPS2018)   
Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference (ICLR2019)  
[First NLP papers] Compositional Continual Language Learning (ICLR2020)  
[First NLP papers] LAMOL: LAnguage MOdeling for Lifelong Language Learning (ICLR2020)  
A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning (ICLR2020)  
Continual learning with hypernetworks (ICLR2020)  
Continual Learning with Adaptive Weights (CLAW) (ICLR2020)  
Coresets via Bilevel Optimization for Continual Learning and Streaming (NeurIPS2020)  
Calibrating CNNs for Lifelong Learning (NeurIPS2020)  
Continual Deep Learning by Functional Regularisation of Memorable Past (NeurIPS2020)   

## THEORETICAL PAPERS
### HARDCORE
A PAC-Bayesian Bound for Lifelong Learning (ICML2014)  
Lifelong learning with non-iid tasks (NIPS2015)  
Lifelong learning with weighted majority votes (NIPS2016)  
Optimal Continual Learning has Perfect Memory and is NP-HARD (ICML2020)  

### PAPERS THAT BASED FROM PRINCIPLES FROM CLASSICAL/THEORETICAL PAPERS
Overcoming Catastrophic Forgetting by Incremental Moment Matching (NIPS2017)  
Lifelong Learning with Dynamically Expandable Networks (ICLR2018)  
PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning (CVPR2018)  
Lifelong Learning via Progressive Distillation and Retrospection (ECCV2018)  
Memory Aware Synapses: Learning what (not) to forget (ECCV2018)  
Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights (ECCV2018)  
End-to-End Incremental Learning (ECCV2018)   
Overcoming Catastrophic Forgetting via Model Adaptation (ICLR2019)  
[Good code base] Efficient Lifelong Learning with A-GEM (ICLR2019)  
Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting (ICML2019)  
Task-Free Continual Learning (CVPR2019)  
Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning (CVPR2019)   
Learning Without Memorizing (CVPR2019)   
Learning a Unified Classifier Incrementally via Rebalancing (CVPR2019)   
Large Scale Incremental Learning (CVPR2019)  
Continual Learning by Asymmetric Loss Approximation with Single-Side Overestimation (ICCV2019)  
Online Continual Learning with Maximal Interfered Retrieval (NeurIPS2019)  
Random Path Selection for Incremental Learning (NeurIPS2019)  
Incremental Learning Using Conditional Adversarial Networks (ICCV2019)  
IL2M: Class Incremental Learning With Dual Memory (ICCV2019)  
Complementary Learning for Overcoming Catastrophic Forgetting Using Experience Replay (IJCAI2019)  
Compacting, Picking and Growing for Unforgetting Continual Learning (NeurIPS2019)  
Mnemonics Training: Multi-Class Incremental Learning without Forgetting (CVPR2020)  
[Meta CL] iTAML : An Incremental Task-Agnostic Meta-learning Approach (CVPR2020)  
Conditional Channel Gated Networks for Task-Aware Continual Learning (CVPR2020)  
Incremental Learning In Online Scenario (CVPR2020)  
Online Continual Learning under Extreme Memory Constraints (ECCV2020)  
REMIND Your Neural Network to Prevent Catastrophic Forgetting (ECCV2020)  
Adversarial Continual Learning (ECCV2020)  
Dark Experience for General Continual Learning: a Strong, Simple Baseline (NeurIPS2020)   
Using Hindsight to Anchor Past Knowledge in Continual Learning(AAAI, 2021)  


## ANALYSIS PAPERS
### GOOD INSIGHTS
Revisiting distillation and incremental classifier learning (ACCV2018)  
Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence (ECCV2018)  
Measuring Catastrophic Forgetting in Neural Networks (AAAI2018)  
A comprehensive, application-oriented study of catastrophic forgetting in DNNs (ICLR2019)  
[Nice summary paper, but the proposed method doesnâ€™t work] GDumb: A Simple Approach that Questions Our Progress in Continual Learning (ECCV2020)   
Understanding the Role of Training Regimes in Continual Learning (NeurIPS2020)   
Anatomy of catastrophic forgetting: Hidden representations and task semantics (ICLR2021)  
Optimization and Generalization of Regularization-Based Continual Learning: a Loss Approximation Viewpoint (arxiv2021)  

## CONTINUAL LEARNING & OTHER SETTINGS
### META CONTINUAL LEARNING, GAN AND FORGETTING, CL IN NLP, RS, ETC.
Meta-learning representations for continual learning (NIPS2019)  
Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning (NeurIPS2020)  
GAN Memory with No Forgetting (NeurIPS2020)   
Improving generalization and stability of generative adversarial networks (IJCNN2019)  
Catastrophic forgetting and mode collapse in GANs (IJCNN2020)  
Meta-Consolidation for Continual Learning (NeurIPS2020)   
Distill and Replay for Continual Language Learning(COLING, 2020)  
Continual Learning for Natural Language Generation in Task-oriented Dialog Systems(EMNLP, 2020) (Findings of ACL)  
ADER: Adaptively Distilled Exemplar Replay Towards Continual Learning for Session-based Recommendation(RecSys, 2020) (Short paper)  
One Person, One Model, One World: Learning Continual User Representation without Forgetting (arxiv2020)  
